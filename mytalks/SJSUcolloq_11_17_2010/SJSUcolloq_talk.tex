\documentclass{beamer}
\usefonttheme[onlymath]{serif}

\newcommand{\Balert}[1]{\textcolor{blue}{#1}}

\mode<presentation>
{   \usetheme{Copenhagen} }

\usepackage{bm,verbatim,graphicx,datetime}
\usepackage{color,braket}
\usepackage[english]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
% Or whatever. Note that the encoding and the font should match. If T1
% does not look nice, try deleting the line with the fontenc.

%% ----------------------
\input ../../documentation/doc_macros.tex

\usdate  % format for \today
\settimeformat{ampmtime}
\date{November 17, 2010}
%%% ---------------


\title[Krylov Subspaces]{Krylov Subspaces and Their Application to Model Order Reduction} 

%\subtitle{} % (optional)

\author[]{Efrem Rensi}
\institute{UC Davis Applied Mathematics}
% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command: 
%\beamerdefaultoverlayspecification{<+->}

% *****************************************************
\begin{document}

\begin{frame}
  \titlepage
  \begin{figure}
  	\centering
  	    \putfig[{324 149}]{0.30}{IO.png}\hfill
	     %\putfig[{1158 1191}]{0.10}{seal_blue-gold.png}
  \end{figure}
\end{frame}


%\begin{comment}%%%%%%%%%%%%%%%

\begin{frame}{Basic Linear Algebra}%
Take $\A\in\C^{N\times N}$ and $r\in\C^{N}$. The matrix-vector product
\[
  \A r \in \C^N
\]
is a vector. 

Example in $\R^3$:
\[
\underbrace{\mat{1& 2& 3\\4& 5& 6\\ 2 &7 &3}}_{\A} \underbrace{\mat{1\\1\\1}}_r
= \mat{6 \\15 \\12 }
\]
\end{frame}

\begin{frame}{Basic Linear Algebra}%
Take $\A\in\C^{N\times N}$ and \emph{block} $R\in\C^{N\times p}$. The product
\[
  \A R \in \C^{N\times p}
\]
is an $N\times p$ block. 

Example in $\R^3$:
\[
\underbrace{\mat{1& 2& 3\\4& 5& 6\\ 2 &7 &3}}_{\A} \underbrace{\mat{1 & 3\\1&2\\1&1}}_R
= \mat{6&10\\15&28\\12&23}
\]
\end{frame}


\begin{frame}{Krylov Sequence}%
Successive applications of operator $\A$ to a start vector $r$ 
\[
r, \A r,  \A \A r, \A \A \A r, \ldots  
\]
result in the \emph{Krylov sequence}
\[
r, \A r,  \A^2r, \A^3r, \ldots  
\]

\end{frame}


\begin{frame}{Krylov Sequence}%
Example:
\begin{itemize}
\item $\underbrace{\mat{1& 2& 3\\4& 5& 6\\ 2 &7 &3}}_{\A}
\underbrace{\mat{1\\1\\1}}_r = \mat{6 \\15 \\12 }$

\item $\A^2r = \mat{1& 2& 3\\4& 5& 6\\ 2 &7 &3} \mat{6 \\15 \\12 }
= \mat{72 \\171 \\153}$

\item $\A^3r = \mat{1& 2& 3\\4& 5& 6\\ 2 &7 &3} \mat{72 \\171 \\153}
= \mat{873 \\2061 \\1800}$
\end{itemize}
\end{frame}


\begin{frame}{Krylov Sequence}%
The Krylov sequence induced by $\A$ and $r$ is 
\[
\underbrace{\mat{1\\1\\1}}_r, \underbrace{\mat{6 \\15 \\12 }}_{\A r}, 
\underbrace{\mat{72 \\171 \\153}}_{\A^2 r}, \underbrace{\mat{873 \\2061 \\1800}}_{\A^3 r},
\underbrace{\mat{10395\\24597\\21573}}_{\A^4 r},\ldots \in \R^3
\]
\end{frame}


\begin{frame}{Krylov Subspace}{Example}%
The $3$rd \emph{Krylov subspace} induced by $\A$ and $r$:
\[
\krylov{3}{\A}{r} = \colspan\left\{r,\A r,\A^2 r \right\}
\]


All of the following are in $\krylov{3}{\A}{r}$
\begin{itemize}
\item $r$
\item $\A^2 r + 2\A r$
\item $ r + 3 \A r + 5 \A^2 r$  
\end{itemize}

In fact, 
\[
c_0 r + c_1 \A r + c_2 \A^2 r  \quad\text{for any}\quad c_0,c_1,c_2\in \C
\]
\end{frame}


\begin{frame}{Krylov Subspace}%
For $\A\in\C^{N\times N}$ and $r\in\C^N$, 
\begin{itemize}
\item Biggest possible Krylov subspace is $N$-th 
\[
\krylov{N}{\A}{r} =  \colspan\left\{r,\A r,\A^2 r,\ldots,\A^{N-1}r \right\} \subseteq \C^N
\]
\end{itemize}

Example:  Recall in $\R^3$
\[
\A = \mat{1& 2& 3\\4& 5& 6\\ 2 &7 &3}, \quad  r = \mat{1\\1\\1}
\]

Krylov sequence is 
\[ 
\alert{
	\underbrace{\mat{1\\1\\1}}_r, \underbrace{\mat{6 \\15 \\12 }}_{\A r}, 
	\underbrace{\mat{72 \\171 \\153}}_{\A^2 r} },
	\underbrace{\mat{873 \\2061 \\1800}}_{\A^3 r}  ,
\ldots
\]
\end{frame}


\begin{frame}{Invariance}{Example}%
\[ 
  \A = \mat{
   -4.8 & 10.6 & -3.8 \\
   -5.8 & 11.6 & -3.8 \\
   -6.7 & 12.4 & -3.7 }, \quad z = \mat{1\\1\\1}
\]

Multiply:
\[
\mat{
   -4.8 & 10.6 & -3.8 \\
   -5.8 & 11.6 & -3.8 \\
   -6.7 & 12.4 & -3.7 } 
 \mat{1\\1\\1} = \mat{2\\2\\2}
\]


\begin{itemize}
\item $\A z = 2z$

\item $( 2, \mat{1\\1\\1})$ is an eigen-pair of $\A$. 
\end{itemize}
\end{frame}



\begin{frame}{Invariance}{Example}%

Krylov sequence induced by $\A$ and $z$ is
\[
\underbrace{\mat{1\\1\\1}}_{z}, \underbrace{\mat{2\\2\\2}}_{2z}, 
\underbrace{\mat{4\\4\\4}}_{4z}, \underbrace{\mat{8\\8\\8}}_{8z}, \ldots, 
\underbrace{\mat{2^j\\2^j\\2^j}}_{2^j z}
, \ldots   
\]

\begin{itemize}
\item $\krylov{n}{\A}{z} = \colspan\{z\} = \colspan \left\{ \mat{1\\1\\1} \right\}$
for any $n$.
\item \emph{Invariant Subspace} with respect to $\A$
\item Eigenspace
\end{itemize}
\end{frame}


\begin{frame}{Invariance}{Example}%
 \[
 (\frac{1}{2},\mat{2\\2\\3}) \quad\text{is another eigen-pair of } \A 
 \]
 
 \[
 \mat{
    -4.8 & 10.6 & -3.8 \\
    -5.8 & 11.6 & -3.8 \\
    -6.7 & 12.4 & -3.7 } 
 \mat{2\\2\\3} = \mat{1\\1\\1.5}
 \]

 Krylov sequence:
 \[
 \mat{2\\2\\3}, \mat{1\\1\\1.5}, \mat{0.5\\ 0.5\\ 0.75}, \mat{0.25\\0.25\\ 0.375},
 \ldots, \mat{2\cdot 2^{-j}\\2\cdot 2^{-j}\\ 3\cdot 2^{-j}}, \ldots
 \]
\end{frame}
 

\begin{frame}{Invariance}{Example}%
\[
\A = \mat{ -4.8 & 10.6 & -3.8 \\ -5.8 & 11.6 & -3.8 \\ -6.7 & 12.4 & -3.7 } 
\]

	\begin{itemize}
		\item Eigenvalues of $\A$ are $2$, $\frac{1}{2}$, (and $1$)
		\item $2$ is the \emph{dominant eigenvalue}, with eigenvector $z_1=\mat{1\\1\\1}$
	\end{itemize}

\end{frame}


\begin{frame}{Krylov Sequence Convergence}
Sequence usually converges to the dominant eigenvector
\bigskip 

\begin{itemize}
	\item $2$ is the \emph{dominant eigenvalue} of $\A$, with eigenvector
	$\mat{1\\1\\1}$

\medskip
\item Generate a Krylov sequence with $\A$ and \emph{almost} any start vector $r$.
Say,
\[ 
\alert{r=\mat{0\\-1\\-4}} = \underbrace{\mat{1\\1\\1}}_{z_1} 
- \underbrace{2\mat{2\\2\\3}}_{2z_2} + \underbrace{\mat{3\\2\\1}}_{z_3}
\]
\end{itemize}
\end{frame}



\begin{frame}{Krylov Sequence Convergence}
Sequence usually converges to the dominant eigenvector
\bigskip 

\begin{itemize}
	\item $2$ is the \emph{dominant eigenvalue} of $\A$, with eigenvector
	$\mat{1\\1\\1}$

\medskip
\item Generate a Krylov sequence with $\A$ and \emph{almost} any start vector $r$.

\[ 
\alert{\A r=\A \mat{0\\-1\\-4}} = \underbrace{\A \mat{1\\1\\1}}_{\A z_1} 
- \underbrace{2\A \mat{2\\2\\3}}_{2\A z_2} + \underbrace{\A \mat{3\\2\\1}}_{\A z_3}
\]
\end{itemize}
\end{frame}




\begin{frame}{Krylov Sequence Convergence}
Sequence usually converges to the dominant eigenvector
\bigskip 

\begin{itemize}
	\item $2$ is the \emph{dominant eigenvalue} of $\A$, with eigenvector
	$\mat{1\\1\\1}$

\medskip
\item Generate a Krylov sequence with $\A$ and \emph{almost} any start vector $r$.

\[ 
\alert{ \A r=\mat{3\\2\\0} } = \underbrace{\mat{2\\2\\2}}_{2z_1} 
 - \underbrace{\mat{2\\2\\3}}_{2\cdot \frac{1}{2}z_2} + \underbrace{\mat{3\\2\\1}}_{z_3}
\]
\end{itemize}
\end{frame}



\begin{frame}{Krylov Sequence Convergence}
Compute $r,\A r,\A^2 r, \A^3 r,\ldots$:
\[
\mat{0\\-1\\-4}, \mat{3\\2\\0},\mat{6\\5\\3.5},\ldots,
\underbrace{\mat{1027\\1026\\1025}}_{\A^{10}r}, \ldots
\]
 
Converges to a multiple of $\mat{1\\1\\1}$ (dominant eigenvector) quickly.
\end{frame}



\begin{frame}{Krylov Sequence Convergence}
Actually, power iterations compute 
\[
 v_1 = \frac{r}{\nrm{}{r}},\quad v_2 =\frac{\A v_1}{\nrm{}{\A v_1}}= \frac{\A^2 r}{\nrm{}{\A^2 r}},
\quad v_3 = \frac{\A v_2}{\nrm{}{\A v_2}}=\frac{\A^3 r}{\nrm{}{\A^3 r}},\quad\ldots
\]

\[
\underbrace{\mat{0\\-0.2425\\-0.9701}}_{v_1},\underbrace{\mat{ 0.8321\\0.5547\\0}}_{v_2},
\underbrace{\mat{0.701\\0.5842\\0.4089}}_{v_3},\ldots,
\underbrace{\mat{0.5779\\0.5774\\0.5768}}_{v_{10}}, \ldots
\]

\end{frame}





\begin{frame}{Krylov Sequence Convergence}
\large{Using Power Iterations:}
\bigskip 

Computing basis for 
\[
\krylov{n}{\A}{r} = \colspan\{r, \A r, \A^2 r, \ldots, \A^{n-1} r \}
\]
\bigskip
using finite precision arithmetic 

\begin{itemize}
\item We quickly get stuck at the dominant eigenvector after a few iterations!

\bigskip
\item (Useful for eigenvalue computation though)
\end{itemize}

\end{frame}


\begin{frame}{Krylov Sequence Convergence}
In general, for $\A\in\C^{ N\times N }$ with 
\begin{itemize}
\item $N$ eigenvalues $|\lambda_1| > |\lambda_2| \geq |\lambda_3| \geq\cdots \geq |\lambda_N|$

\item eigenvectors $z_1,z_2,\ldots,z_N$, 
\end{itemize}

\bigskip
For any start vector $r\in\C^N$

\begin{align*}
\A^k r &= \A^k \left(a_1 z_1 + a_2 z_2 + \cdots + a_k z_k \right)\\
&= a_1\lambda_1^k\left( z_1 + \sum\frac{a_j}{a_1}\alert{\left( \frac{\lambda_j}{\lambda_1} \right)^k} z_j \right)
\end{align*}
\end{frame}


\begin{frame}{Basis for $\krylov{n}{\A}{r}$}
Assuming we don't get stuck in an invariant subspace, Krylov vectors 
\[
\left\{ r, \A r,  \A^2r, \A^3r, \ldots, \A^{n-1}r   \right\}
\]

\bigskip
\begin{itemize}
\item Are linearly independent, and span $\krylov{n}{\A}{r}$

\medskip
\item Form a bad basis for $\krylov{n}{\A}{r}$
\end{itemize}
\end{frame}


\begin{frame}{Arnoldi Process}{Generates basis for Krylov subspace}
\bigskip

\bigskip
Arnoldi process computes orthogonal basis matrix 
$V_n = \left[ v_1\, v_2\, ...\, v_n \right]$ for Krylov subspace $\krylov{n}{\A}{r}$:
  
  \medskip
  \begin{itemize}
	   \item $v_1 = r/\|r\|$
	   \item $v_2 = \left(\A v_1\ \text{orthogonalized against}\  v_1\right)$
	   \\ $\vdots$ 
	   \item $v_n = \left(\A v_{n-1}\  \text{orthogonalized against}\  
			      \{v_1,v_2,\ldots ,v_{n-1}\}\right)$	
   \end{itemize}
\end{frame}



\begin{frame}{Arnoldi Process}{Computationally expensive for large $N$} 
  The $n$-th iteration of Arnoldi 
\begin{align*}	
   v_{n+1} &\approx \left(\A v_{n}\ \text{orthogonalized against}\ \{v_1,v_2,\ldots ,v_{n}\}\right)\\
	&= \A v_n - \alpha_1 v_1 - \alpha_2 v_2 - \cdots - \alpha_n v_n
\end{align*}

where \[ \alpha_j = \frac{v_j^H v_n}{\nrm{}{v_j}} \] 
  
	\bigskip
For large $N$  ($\approx 10^6$)
\begin{itemize}
 \item computing each $\alpha_j$ requires $\approx2N$ scalar multiplications \& additions.
 \item Computing $v_n\in\C^N$ grinds to a halt with increasing $n$\ !
\end{itemize}

\end{frame}



%%%%%%%%%%%%%%%    My Research    %%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Application: RCL Circuit Simulation}
	\begin{figure}
	 	    \centering
	  	    \putfig[{102 127}]{0.2}{images.jpeg}
	\end{figure}
	\begin{itemize}
	\item Why simulate a circuit?
	\end{itemize}
\end{frame}




\begin{frame}{VLSI Circuit Model Reduction}
Example: RCL circuit
	\begin{figure}
	 	    \centering
	  	    \putfig[{723 543}]{0.70}{ttt.png}
	\end{figure}
\end{frame}


\begin{frame}{Circuit Equations}{\textcolor{yellow}{(You Don't Need To Understand This!!)}}
Equations determining any circuit determined via
\smallskip

\begin{itemize}
 \item Kirchhoff's current and voltage laws (KCLs, KVLs)
 \item Branch Constitutive Relations (BCRs)
\end{itemize}

\smallskip
KCLs, KVLs of the circuit can be stated as
\begin{equation*}
\As i_\varepsilon = 0 \quad \text{and} \quad \As^T v = v_\varepsilon
\end{equation*}
with \emph{incidence matrix}
\begin{equation*}
\As = \mat{\As_r & \As_c & \As_l & \As_v & \As_i}, 
\end{equation*}
and \emph{current, voltage} vectors
\[
i_\varepsilon = \mat{i_r\\i_c\\i_l\\i_v\\i_i}, v_\varepsilon = \mat{v_r\\v_c\\v_l\\v_v\\v_i}.
\]  

\end{frame}

\begin{frame}{Circuit Equations}{\textcolor{yellow}{(You Don't Need To Understand This!!)}}
Equations determining any circuit determined via
\smallskip

\begin{itemize}
 \item Kirchhoff's current and voltage laws (KCLs, KVLs)
 \item Branch Constitutive Relations (BCRs)
\end{itemize}

\smallskip
BCRs of the circuit can be stated as
\[
v_r(t) = Ri_r(t),\quad i_c(t)=C\frac{d}{dt}v_c(t),\quad v_l(t)=L\frac{d}{dt}i_l(t)
\]  

\begin{itemize}
 \item $R$, $C$, and $L$ are diagonal matrices containing resistances, capacitances, 
inductances of components 
\end{itemize} 
\end{frame}

\begin{frame}{RCL Circuit Equations}{Realization}
Then we formulate \emph{Realization} of the circuit: 
\bigskip

Block matrices
\[
A = \mat{A_{11}& -\As_l&-\As_v\\ \As^T_l &0&0\\A^T_v&0&0}, \quad
 E= \mat{E_{11}&0&0\\ 0&L&0\\ 0&0&0}, \quad
 B = \mat{\As_i&0\\ 0&0\\ 0&-I},
\]
where 
\[
A_{11}= -\As_r R^{-1}\As_r^T \quad\text{and}\quad E_{11} = \As_c C\As_c^T.
\]

\begin{itemize}
\item $A,E\in\R^{N\times N}$ and $B\in\R^{N\times \nin}$ sparse, large ($N > 10^6$).
\item Any $A$, $E$, $B$ having this structure determine a RCL circuit.
\end{itemize}

\end{frame}




\begin{frame}{Unreduced Model (RCL Circuit)}{Descriptor System}
   \emph{Input-Output} system represented as a system of 
   Differential Algebraic Equations (DAEs)
   \begin{equation*}  
   \begin{array}{c}
   u_1(t)\quad\longrightarrow \\ u_2(t)\quad\longrightarrow\\ \vdots\\u_\nin(t)\quad\longrightarrow
   \end{array}
  \quad
  \boxed{\Balert{%
 	\begin{aligned}
 	    \\
 			\quad E x' &=  Ax + Bu\quad\\
 			y &= B^Tx,\\
 			\\
 	\end{aligned}		
      }
	}
  \quad	
		 \begin{array}{c}
	 \longrightarrow\quad y_1(t)\\ \longrightarrow\quad y_2(t)\\ \vdots\\\longrightarrow\quad y_\nin(t)
   \end{array} 
 \end{equation*}
 where $A,E\in\R^{N\times N}$ (possibly singular), $B\in\R^{N\times \nin}$.
 
 \begin{itemize}
   \item $u(t),y(t)\in\R^p$ input,output vectors
   \item $x(t)\in\R^N$ represents internal state space (to be reduced).
  \item \emph{Behavior} of model: \alert{$y(t)=F(u(t))$}
   \end{itemize}
 \end{frame}


\begin{frame}{Reduced Order Model (ROM) via Projection}
   System of DAEs of the same form
   \begin{equation*}  
   \begin{array}{c}
   u_1(t)\quad\longrightarrow \\ u_2(t)\quad\longrightarrow\\ \vdots\\u_\nin(t)\quad\longrightarrow
   \end{array}
  \quad
  \boxed{\Balert{%
 	    \begin{aligned}
 	    \\
 			\quad \En x' &=  \An x + \Bn u\quad\\
 			y &= \Bn^Tx\\
 			\\
 		\end{aligned}	
      }
  	}
  \quad	
		 \begin{array}{c}
	 \longrightarrow\quad y_1(t)\\ \longrightarrow\quad y_2(t)\\ \vdots\\\longrightarrow\quad y_\nin(t)
   \end{array} 
 \end{equation*}
 
 \[
 	\An := V_n^TAV_n, \quad \En := V_n^TEV_n \quad\in \R^{n\times n}
 \]
 
 \[
 	\Bn := V_n^T\Bn \in\R^{n\times \nin},
 \]
with state-space dimension $n \ll N$ and $V_n\in\R^{N\times n}$ is
basis for some ideal space.  
\end{frame}






\begin{frame}{Transfer Function}{Relates Output directly to Input in Frequency Domain}
Original system:
  \begin{align*}
   	E x' &=  Ax + Bu\\
   	 y &= B^Tx.
 \end{align*}
 Applying the Laplace transform,
  \begin{align*}
      sEX(s) &=  AX(s) + BU(s)\\
      Y(s) &= B^TX(s).
 \end{align*}

In the frequency domain,
\[
Y(s) = \Balert{B^T(sE-A)^{-1}B} U(s) \equiv \Balert{H(s)}U(s).
\]
\end{frame}



\begin{frame}{Transfer Function}{Relates Output directly to Input}
    In the frequency domain, $Y(s) = H(s)U(s)$ with \emph{transfer~function}
 	 \[
 		H(s) =  B^T(sE-A)^{-1}B \quad\in\quad (\C\cup\infty)^{\nin \times \nin}
 	\]
 	\begin{figure}
	     \centering
	     	\putfig{0.55}{ex1841s1_urm_tfunc.png}
	     	\caption{$\|H(s)\|$ vs. frequency for $N=1841$ test model}
     \end{figure}
 \end{frame}
 



 \begin{frame}{Transfer Function}{Domain $S\in\C$}
  We consider $H(s)$ over $s\in S$.
  \[
  		S = 2\pi if,\quad f\in\left[f_\mathrm{min},f_\mathrm{max}\right]
  \]
     \begin{figure}
     \centering
     	\putfig{0.48}{ex1841s1_urm_tfuncS.PNG}\hfill
     	\putfig[{570 402}]{0.48}{Splane2.png}
     	%\caption{}
     \end{figure}
 \end{frame}


\begin{frame}{Pole decomposition of model}{Example: poles of a size $N=1841$ test model}
	\begin{figure}[htbp]
		\centering
			\putfig[{735 420}]{.90}{ex1841s1_poles_semilog.png}
			\\ $\log_{10}$ scale on Re axis. Dot size indicates dominance.
	\end{figure}
\end{frame}




\begin{frame}{Transfer Function}{Input $\rightarrow$ Output Map in Frequency Domain}
    In the frequency domain, $Y(s) = H(s)U(s)$ with transfer~function 
 	\begin{align*}
 		H(s) &=  B^T(sE-A)^{-1}B \quad\in\quad (\C\cup\infty)^{\nin \times \nin}
 	\end{align*}
 	For the reduced model,
 	\begin{align*}
	 	H_n(s) &=  \Bn^T(s\En-\An)^{-1}\Bn \quad\in\quad (\C\cup\infty)^{\nin \times \nin}
 	\end{align*}
 	
 	\[
 		H_n(s)\approx H(s) \quad\Longleftrightarrow\quad
 		\text{`Good' Reduced Order Model}
  	\]
\end{frame}
 


\begin{frame}{Local Convergence of ROM}
    Reduced order transfer function 
 	 \[
		H_n(s) =  \Bn^T(s\En-\An)^{-1}\Bn 
 	\]
 	\begin{figure}
	     \centering
	     	\putfig{0.60}{ex308s1_loc15_tfunc.png}
	     	\caption{$\|H_{15}(s)\|$ converges near placement of $s_0$}
     \end{figure}
 \end{frame}
 


\begin{frame}{Moment Matching}
 	Expressed as Taylor series expansion about $\xp\in\C$:
 	\begin{align*}
 		\text{Original:}\quad H(s) &= \sum_{j=0}^\infty (s-\xp )^jM_j \\
		\medskip
 		\text{ROM:}\quad H_n(s) &= \sum_{j=0}^\infty (s-\xp )^j\widetilde{M}_j\\
 	\end{align*}

 ROM matches $n$ moments about $\xp$ if
 $\widetilde{M}_j=M_j$ for $j=0,1,\ldots,n-1$.

\end{frame}






\begin{frame}{Transfer function}{Single-matrix formulation}
Choose expansion point $\xp\in\C$, re-write $H(s)$ as  
	\begin{align*}
			H(s) &= B^T(sE-A)^{-1}B\\
				&= B^T\left(I-(s-\xp)\alert{\A}\right)^{-1}\alert{R} 
	\end{align*}
(\alert{\emph{Single matrix formulation}}), where 
\[ \A := -(\xp E-A)^{-1}E\quad\text{and}\quad R := (\xp E-A)^{-1}B. \]
\end{frame}




\begin{frame}{Moments of the transfer function about $\xp $}%
 
  Via Neumann (geometric series) expansion, 
  		\begin{align*}
  			H(s) &= B^T\left(I-(s-\xp )\A\right)^{-1}R \\
  				 &= B^T\left(\sum_{j=0}^\infty (s-\xp)^j\A^j\right)R\\
  				 &= \sum_{j=0}^\infty (s-\xp)^j B^T\Balert{\A^j R}
  		\end{align*}     		
  \begin{itemize}
  		\item This the Taylor series expansion of $H(s)$ about $\xp$.
  		\item Recall Block-Krylov sequence
  		      \[ R,\A R,\A^2R,\ldots \Balert{\A^j R},\ldots\]
  \end{itemize}   
\end{frame}
 
 
 
 
\begin{frame}{Moment matching}{}
...suggests \alert{$n$-th Block-Krylov subspace}
\[ 
  \krylov{n}{\A}{R}:= \colspan\left\{R,\A R,\A^2R,\ldots,\A^{n-1}R\right\}.
\]

For $V\in\R^{N\times n}$ such that
 \[  \krylov{n}{\A}{R} \subseteq \text{range}\,V, \]
 	
ROM via projection on to $V$ matches $n$ moments about $\xp$.
\[
\Bn^T\widetilde{\A}^j \widetilde{R} = B^T\A^j R\quad\text{for}\quad j=0,1,2,\ldots,n-1
\]
\begin{itemize}
\item because $\Bn^T \widetilde{\A}^j \widetilde{R} 
= B^T \Balert{V \widetilde{\A}^j \widetilde{R}} = B^T\Balert{\A^j R }$  
\end{itemize}
\end{frame}


\begin{frame}{Reduced Order Model (ROM) via Projection}
   System of DAEs of the form
   \begin{equation*}  
   \begin{array}{c}
   u_1(t)\quad\longrightarrow \\ u_2(t)\quad\longrightarrow\\ \vdots\\u_\nin(t)\quad\longrightarrow
   \end{array}
  \quad
  \boxed{\Balert{%
 	    \begin{aligned}
 	    \\
 			\quad \En x' &=  \An x + \Bn u\quad\\
 			y &= \Bn^Tx\\
 			\\
 		\end{aligned}	
      }
  	}
  \quad	
		 \begin{array}{c}
	 \longrightarrow\quad y_1(t)\\ \longrightarrow\quad y_2(t)\\ \vdots\\\longrightarrow\quad y_\nin(t)
   \end{array} 
 \end{equation*}
 
 \[
 	\An := V_n^TAV_n, \quad \En := V_n^TEV_n \quad\in \R^{n\times n}
 \]
 
 \[
 	\Bn := V_n^T\Bn \in\R^{n\times \nin},
 \]
with $n \ll N$ and $V_n\in\R^{N\times \eta}$ such that 
 \[  \krylov{n}{\A}{R} \subseteq \text{range}\,V_n. \]
\end{frame}


\begin{frame}{How Much Reduction Possible?}
Experimentally, on the order of $2\aleph_0$

\begin{itemize}
\item But this can be improved. (current ongoing research)
\end{itemize}

\end{frame}

\begin{frame}{The End}
\center{Thanks a lot SJSU Math!}
\begin{figure}
  	\centering
  	    \putfig[{611 350}]{0.30}{sjsu_logo_color_web.jpg}
  \end{figure}
\end{frame}

 
%\end{comment}

%\nocite{grimme97}
%\bibliographystyle{plain} 
%\bibliography{erensi_refs}

\end{document}


